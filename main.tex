\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}

\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xeCJK}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}




\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{Condition}

\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}




\begin{document}

\begin{frontmatter}

\title{a Randomization Test for Mean Vector In High Dimension}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
    Testing mean vector is a fundamental problem in statistics.
    Many test procedures for high dimensional mean vector have been proposed in the literature.
\end{abstract}

\begin{keyword}
\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers


\section{Introduction}

Consider i.i.d.\ random sample $X_{1},\ldots,X_{n}\in \mathbb{R}^p$ which has means $\mu={(\mu_1,\ldots,\mu_p)}^T$ and covariance matrix $\Sigma$. The one sample testing problem
\begin{equation}\label{ourHy}
    H_0:\mu=0_p\quad \textrm{versus} \quad H_1:\mu\neq 0_p
\end{equation}
has been extensively studied by many researchers.
A classical test statistic is Hotelling's $T^2$, 
    $
    T_{1}=n\bar{X}^T S^{-1}\bar{X}
    $,
where $\bar{X}$ and $S=(n-1)^{-1}\sum_{i=1}^n (X_i-\bar{X}) (X_i-\bar{X})^T$ are the sample mean vector and sample covariance matrix, respectively.
However, the high-dimensional case $d>n$ invalidates the Hotelling's test.
\cite{Bai1996Efiect} is the first paper modifying Hotelling's $T^2$ statistic for testing~\eqref{ourHy} in high dimensional setting.
This seminal paper removed $S^{-1}$ from $T_1$ and proposed a test statistic based on
$
T_2=\bar{X}^T\bar{X}
$.
Many subsequent papers relaxed the assymptions and generalized the idea of~\cite{Bai1996Efiect}.
For example,~\cite{Srivastava2009A} proposed a test based on
$
T_3=\bar{X}^T {[\mathrm{diag}(S)]}^{-1} \bar{X}
$,
where $\mathrm{diag}(S)$ is a matrix with diagonal elements equal to that of $S$ and off-diagonal elemnts equal to $0$.
\cite{Chen2010A} proposed a test based on
$
T_4=\sum_{i\neq j}X_i^T X_j
$.~\cite{Wang2015A} proposed a test based on $T_5=\sum_{i\neq j}Y_i^T Y_j$, where $Y_i$ is defined as $Y_i=X_i/\|X_i\|$ if $X_i\neq 0$; and $Y_i=0$ if $X_i=0$.

Note that the test statistics $T_2$--$T_5$ all can be written as a generalized quadratic form of data, see~\cite{Jong1987A}, and their theoretical proves mostly rely on an application of martingale central limit theorem (MCLT).

The critical value of existing high dimensional tests are often determined by asymptotical distribution. 
We will call it asymptotic method.
Asymptotic method can guarantee the test level asymptotically.
However, in many real world problems, e.g., gene testing (see~\cite{efron2007on}), sample size $n$ may be very small.
Hence the Type I error rates of Asymptotic method may be far away from nominal levels in this case. 


The idea of randomization test dates back to~\cite{Fisher}, which is a tool to determine the critical value for a given test statistic.
\cite{Romano1990On} described a general construction of the randomization test.
In high dimension setting, randomization test is widely used in applied statistics, see~\cite{efron2007on}.
It's strongth is in that the resulting test procedure has exact level under mild condition.
Although there are many papers give theoretical analysis for fixed $p$ case,
to the best of our knowledge, there's no existing theoretical work concerning high dimension setting.

Suppose the distribution of $X_i$ is symmetric about $0$ under the null hypothesis. $T$ is certain test statistic for testing~\eqref{ourHy}.
Let $\epsilon_1,\ldots,\epsilon_n$ be i.i.d.\ Rademacher variables ($\Pr(\epsilon_i=1)=\Pr(\epsilon_i=-1)=1/2$) which are independent of the data.
The conditional distribution
\begin{equation}\label{ranDis}
    \mathcal{L}(T(\epsilon_1 X_1,\ldots,\epsilon_i X_i,\ldots,\epsilon_n X_n)|X_1,\ldots,X_n)
\end{equation}
is the uniform distribution on $2^n$ values.
The critical value of the randomization test is defined as the $1-\alpha$ quantile of the above distribution.
 More specifically, the test function equals to $1$ or $0$ if $T(X_1,\ldots, X_n)$ is greater or not greater than the critical value. The resulting test is a level $\alpha$ test if the distribution of $X_i$ is symmetric under null.
 By some refinement of the test function when $T(X_1,\ldots,X_n)$ equals to the critical value, the test is exact, see~\cite{Romano1990On}.
Since such extreme case occurs with little probability $2^{-n}$, the refinement is often dropped in practice, which only losses  a little power.

Equivalently, the randomization test can be implemented by $p$-value. Define 
\begin{equation*}
    p(X_1,\ldots, X_n)=
    \Pr(T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)\geq T( X_1,\ldots,X_n)|X_1,\ldots,X_n).
\end{equation*}
The randomization test rejects the null hypothesis if $p(X_1,\ldots, X_n)\leq \alpha$. 



 It's easy to see that the randomization version of~\cite{Bai1996Efiect}'s test and~\cite{Chen2010A}'s test are both equivelent to the randomization test based on
\begin{equation}\label{Statistic}
    T(X_1,\ldots,X_n)=\sum_{j<i}X_i^T X_j.
\end{equation}
In this paper, we will study this special statistic for illustration.
From now on, $T(X_1,\ldots,X_n)$ will refer to~\eqref{Statistic}.
Other quadratic based statistic can be studied by similar method.

Our results show that even if the null distribution is not symmetric, the randomization test is still asymptotically exact under mild assumptions. 
We also give the local asymptotic power.



%Existing works of randomization test mainly focus on single variate case. A recent work of EunYi Chung consider multivariate case.


In fixed $p$ setting, it's well known that randomization test has much higher time complexity than asymptotic method, which  historically  hampered it's use. 
Surprisingly, in high dimension setting the randomization test can be implemented as efficiently as asymptotic method.


%There's no need to estimate the variance of the statistic.

Maybe the most widely used randomization method is the two sample permutation test.
As~\cite{Romano1990On} pointed out, the asymptotic property of randomization tests depends heavily on the particular problem and, the two sample case is quite distinct from the one sample case.
The method used in this paper can not be applied to permutation test.

\section{Randomization test}
We assume, like~\cite{Chen2010A} and~\cite{Bai1996Efiect}, the following multivariate model:
\begin{equation}\label{chenC1}
    X_i=\mu+\Gamma Z_i\quad \textrm{for}\, i=1,\ldots,n,
\end{equation}
where $\Gamma$ is a $p\times m$ matrix for some $m\geq p$ such that $\Gamma_i\Gamma_i^T=\Sigma_i$ and ${\{Z_{i}\}}_{i=1}^n$ are $m$-variate i.i.d.\ random vectors satisfying $\mathrm{E}(Z_i)=0$ and $\mathrm{Var}(Z_i)=I_m$, the $m\times m$ identity matrix. Write $Z_i={(z_{i1},\ldots,z_{im})}^T$, we assume $\mathrm{E}(z_{ij}^4)=3+\Delta<\infty$ and
\begin{equation}\label{chenC2}
    \mathrm{E}(z_{il_1}^{\alpha_1}z_{il_2}^{\alpha_2}\cdots z_{il_q}^{\alpha_q})=\mathrm{E}(z_{il_1}^{\alpha_1})\mathrm{E}(z_{il_2}^{\alpha_2})\cdots \mathrm{E}(z_{il_q}^{\alpha_q})
\end{equation}
for a positive integer $q$ such that $\sum_{l=1}^q \alpha_l\leq 8$ and $l_1\neq l_2\neq \cdots \neq l_q$.

In the following, $T(X_1,\ldots, X_n)$ will be specialized to~\eqref{Statistic}.

An important assumption in~\cite{Chen2010A} is
    $\mathrm{tr}(\Sigma^4)=o\{\mathrm{tr}^2(\Sigma^2)\}$,
which is equavelent to 
\begin{equation}\label{chenC3}
    \frac{\lambda_{\max}(\Sigma)}{\sqrt{\mathrm{tr}\Sigma^2}}\to 0.
\end{equation}
Although Chen and Qin's results is for two sample case, their results can be proved similarly for one sample case. We restate their theorems:
\begin{theorem}\label{theoremChen}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and local alternatives
    \begin{equation}\label{mu1}
        \mu^T \Sigma\mu=o(n^{-1}\mathrm{tr}\Sigma^2),
    \end{equation}
    we have
    \begin{equation}
        \frac{T(X_1,\ldots,X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}\xrightarrow{\mathcal{L}}N(0,1).
    \end{equation}
\end{theorem}
And
\begin{theorem}\label{theoremChen2}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and     \begin{equation}\label{mumu1}
     n^{-1}\mathrm{tr}(\Sigma)^2   =o(\mu^T \Sigma\mu),
    \end{equation}
    we have
    \begin{equation}
        \frac{T(X_1,\ldots,X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{{(n-1)}^2 n \mu^T \Sigma\mu}}\xrightarrow{\mathcal{L}}N(0,1).
    \end{equation}
\end{theorem}

We will call the conditional distribution
\begin{equation*}
        \mathcal{L}\Big(\frac{T(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big)
\end{equation*}
the randomization distribution.
Let $\xi^*_{\alpha}$ be the $1-\alpha$ quantile of the randomization distribution.
Then the test function $\phi(X_1,\ldots,X_n)$ of randomization test equals to $1$ when 
$$
\frac{T(X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}> \xi^*_{\alpha}
$$
and equals to $0$ otherwise.
Since $\xi^*_{\alpha}$ relies on data, the rejection region is determined by not only  $T(X_1,\ldots,X_n)$ but also randomization distribution.

To study the asymptotic property of $\xi^*_{\alpha}$, we need to derive the asymptotic behavior of randomization distribution.
Since the randomization distribution itself is random, we need to define in what sence the convergence is. Let $F$ and $G$ be two distribution function on $\mathbb{R}$ and the Levy metric $\rho$ of $F$ and $G$ is defined as
\begin{equation*}
    \rho(F,G)=\inf\{\epsilon:F(x-\epsilon)-\epsilon\leq G(x)\leq F(x+\epsilon)+\epsilon\quad  \textrm{for all}\quad x\}.
\end{equation*}
It's well known that $\rho(F_n,F)\to 0$ if and only if  $F_n\xrightarrow{\mathcal{L}}F$.
Our first result is:


%Our asymptotic results need a condition stronger than~\eqref{mu1}.
\begin{theorem}\label{shaziCLT}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and 
    \begin{equation}\label{mu2}
        \mu^T\mu=o\big(\sqrt{\mathrm{tr}{\Sigma}^2}\big),
    \end{equation}
    we have that
    \begin{equation*}
        \rho\Big(\mathcal{L}\Big(\frac{T_2(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big),N(0,1)\Big)\xrightarrow{P} 0.
    \end{equation*}
\end{theorem}
Under Theorem~\ref{shaziCLT}'s conditions, the randomization distribution is asymptotically a normal distribution. 
It's also interesting to understand the behavior of randomization distribution when condition~\eqref{mu2} is not valid. We have the following asymptotic result:

\begin{theorem}\label{farT}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and
    \begin{equation}\label{mu3}
       \sqrt{\mathrm{tr}{\Sigma}^2} =o\big(\mu^T\mu\big),
    \end{equation}
    we have
    \begin{equation}
        \rho\Big(\mathcal{L}\Big(\frac{T(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big),\frac{\sqrt{2}}{2}\big(\chi^2_1-1\big)\Big)\xrightarrow{P} 0.
    \end{equation}
\end{theorem}

Once the asymptotic distribution of the randomization distribution is obtained, the asymptotic behaivior of $\xi_{\alpha}^*$ can be derived immediately.
Let $\Phi(\cdot)$ be the cumulative distribution function (CDF) of standard normal distribution, we have
\begin{corollary}\label{corollaryQuan}
    Under the conditions of Theorem~\ref{shaziCLT}, $\xi_{\alpha}^*\xrightarrow{P} \Phi(1-\alpha)$.
\end{corollary}

\begin{corollary}\label{corollaryQuan2}
    Under the conditions of Theorem~\ref{farT},
    $\xi_{\alpha}^*\xrightarrow{P}\frac{\sqrt{2}}{2}(\Phi^{-1}(1-\frac{\alpha}{2})-1)$.
\end{corollary}


Now we are ready to derive the asymptotic power of randomization test.
Since the limit property of $T$ is different under~\eqref{mu1} and~\eqref{mumu1}.
The following two theorems give the power under~\eqref{mu1} and~\eqref{mumu1}, separately.

\begin{theorem}\label{theoremPower}
    Suppose conditions~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and~\eqref{mu1} holds.

    If~\eqref{mu2} holds,
    \begin{equation*}
            \Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)
            =
            \Phi(-\Phi^{-1}(1-\alpha)+\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}})+o(1).
    \end{equation*}

    If~\eqref{mu3} holds,
    \begin{equation*}
            \Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\to 1.
    \end{equation*}
\end{theorem}

\begin{theorem}\label{theoremPower2}
    Under \eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3}~\eqref{mumu1} and either~\eqref{mu2} or~\eqref{mu3},
    \begin{equation*}
            \Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)
            =
            \Phi(\frac{\sqrt{n}\mu^T\mu}{2\sqrt{\mu^T \Sigma \mu}})+o(1),
    \end{equation*}
\end{theorem}
\begin{remark}
    The theorem doesn't assume that the distribution of $X_i$ is symmetric under null. Hence Theorem~\ref{theoremPower} shows that the level of randomization test is robust agianst asymmetry.
\end{remark}

%\begin{remark}
    %When $\alpha=0.05$, $\Phi^{-1}(1-\alpha)\approx 1.645$ and $\frac{\sqrt{2}}{2}(\Phi^{-1}(1-\frac{\alpha}{2})-1)\approx 0.689$.
%\end{remark}

\begin{remark}
    Neither~\eqref{mu1} or~\eqref{mu2} implies the other one.
    For example, suppose $\Sigma=I_p$, then~\eqref{mu1} is equivalent to $\mu^T\mu=o(p/n)$ and~\eqref{mu2} is equivalent to $\mu^T \mu =o(\sqrt{p})$.
    In this case, if $\sqrt{p}/n\to 0$, then~\eqref{mu1} implies~\eqref{mu2}; conversely, if $\sqrt{p}/n\to \infty$, then~\eqref{mu2} implies~\eqref{mu1}.
\end{remark}

%\begin{remark}
%Chen's test power is
    %\begin{equation*}
            %\Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\\
            %=
            %\Phi(-\Phi^{-1}(1-\alpha)+\frac{\sqrt{n}\mu^T\mu}{2\sqrt{\mu^T \Sigma \mu}})+o(1),
    %\end{equation*}
%\end{remark}

The randomization test rejects the null when 
$$
{T(X_1,\ldots, X_n)}>{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}} \xi^*_{\alpha}.
$$
The asymptotic method rejects the null when
$$
{T(X_1,\ldots, X_n)}>\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2} \Phi^{-1}(1-\alpha).
$$

Note that the larger the reject region, the more powerful the test is. Thus we compare 
$
{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}} \xi^*_{\alpha}
$
and
$
\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\hat{\Sigma}^2} \Phi^{-1}(1-\alpha)
$. Suppose $\mathrm{tr}\hat{\Sigma}^2$ is a ratio consistent estimator of $\mathrm{tr}\Sigma^2$. By Lemma~\ref{ratioLemma} in appendix,
\begin{equation*}
    \frac{{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}} \xi^*_{\alpha}}
    {\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\hat{\Sigma}^2} \Phi^{-1}(1-\alpha)}=(1+o_P(1))
    \frac{{\sqrt{\mathrm{tr}{(\Sigma+\mu\mu^T)}^2}} \xi^*_{\alpha}}
    {\sqrt{\mathrm{tr}\Sigma^2} \Phi^{-1}(1-\alpha)},
\end{equation*}
which tends to $1$ when~\eqref{mu2} holds, and tends to $\infty$ when~\eqref{mu3} holds.
Hence randomization may loss some power.



\section{Algorithm}

The quantile $\xi^*_\alpha$ might not be computationally feasible, since the randomized test statistic is (conditionally) uniformly distributed on $2^n$ different values.
In practice, randomization test is often realized through an approximation of $p$-value.
More specifically, we can sample from randomization distribution by generate $\epsilon_1,\ldots,\epsilon_n$ and compute $T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)$.
Repeat $M$ times for a large $M$ and we obtain $T_i^*$, $i=1,\ldots,M$.
%Denote $\xi_i=\mathbf{1}_{\{T_i^*\geq T_0\}}$.
Then  $\sum_{i=1}^M \mathbf{1}_{\{T_i^*\geq T_0\}}/M$ is an approximation of the $p$-value 
$$p(X_1,\ldots,X_n)=\Pr(T_i^*\geq T_0|X_1,\ldots,X_n).$$
Hence we reject the null if $\sum_{i=1}^M \mathbf{1}_{\{T_i^*\geq T_0\}}/M\leq \alpha$.

To be more efficient, we can accept the null once the sum of $\xi_i$ exceeds $M\alpha$ and reject the null once the sum of $1-\xi_i$ reaches $M(1-\alpha)$.

Note that once we have obtained $X_i^T X_j$, the computation of $T_i^*$ has only complexity $O(n^2)$.
The total complexity is thus $O(n^2 (p+M))$. Hence randomization test is very efficient in high dimension setting.
For example, when $M=p$, time spent by randomization is at most twice of that of asymptotic method.
This is different from low dimension setting where randomization is a lot slower than asymptotic method.



%We shall choose $M$ large enough such that 
%$$\Pr\Big(\Big|\frac{1}{M}\sum_{i=1}^M \xi-p(X_1,\ldots,X_n)\Big|>t|X_1,\ldots,X_n\Big)$$
%is smaller than $\epsilon$, where $t$ and $\epsilon$ are specified. By Hoeffding's inequality,
%\begin{equation*}
    %\Pr\Big(\Big|\frac{1}{M}\sum_{i=1}^M \xi-p(X_1,\ldots,X_n)\Big|>t|X_1,\ldots,X_n\Big)\leq 2e^{-2Mt^2}.
%\end{equation*}
%Hence we choose $M=\big[\frac{1}{2t^2}\log(\frac{2}{\epsilon})\big]+1$.






\begin{algorithm}
    \caption{Randomization Algorithm}
\label{theAlgorithm}
    \begin{algorithmic}
        \REQUIRE  $\alpha$, $M$
        \STATE Set $A\gets 0$, $T_0\gets T(X_1,\ldots,X_n)$.
        \STATE Compute $X_i^T X_j$ for $1\leq j< i\leq n$
        \FOR{$i=1$ to $M$}
            \STATE Generate $\epsilon_1,\ldots,\epsilon_n$ and compute $T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)=\sum_{j<i} X_i^T X_j\epsilon_i\epsilon_j$.
            \IF{$T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)>T_0$}
            \STATE $A\gets A+1$
            \ENDIF
            \IF{$A> M\alpha$}
            \RETURN{Accept}
            \ENDIF
            \IF{$i-A\geq M(1-\alpha)$}
            \RETURN{Reject}
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\section{Simulation studies}
In this section, we report some simulation performance of the randomization test in various setting.
The competing method is asymptotic method.

Mean structure:

sparse

dense


Innovation structure:

moving average model: in model~\eqref{chenC1}, we set $m=p+k$ and ${(\Gamma)}_{ij}=\rho_{j-i}$ for $j=i,\ldots, i+k$ and ${(\Gamma)}_{ij}=0$ otherwise. More precisely,
\begin{equation}
    X_{ij}=\sum_{l=0}^k \rho_{l}Z_{i,j+l}
\end{equation}
for $i=1,\ldots, n$ and $j=1,\ldots, p$. Here $Z_{ij}$'s are i.i.d.\ random variables with distribution $F$ for $i=1,\ldots, n$ and $j=1,\ldots, p+k$. 

Like~\cite{Chen2010A}, we consider two different $F$. One is $N(0,1)$, and the other is centralized Gamma$(4,1)$ so that it has zero mean. We set $k=5$ and $k=p$.

factor model:


Let $c=\sqrt{n(n-1)}\mu^T \mu /\sqrt{2\mathrm{tr}\Sigma^2}$ be the signal to noise ratio.
We plot $c$ versus power.

\section{Appendix}
\paragraph{CLT for quadratic form of Rademacher varaibles}
The proof of the Theorem~\ref{shaziCLT} is based on a CLT of the quadratic form of Rademacher variables. 
Such a CLT can be also used to study the asymptotic behavior of many other randomization test.
 Let $\epsilon_1,\ldots,\epsilon_n$ be indepent Rademacher  variables. 
 Consider quadratic form $W_n=\sum_{1\leq j<i\leq n} a_{ij}\epsilon_i \epsilon_j$, where $\{a_{ij}\}$ are nonrandom numbers. Here $\{\epsilon_i\}$ and $\{a_{ij}\}$ may depend on $n$, a parameter we suppress.
 Obviously, $\mathrm{E}(W_n)=0$ and $\mathrm{Var}(W_n)=\sum_{1\leq j<i\leq n} a_{ij}^2$.

 \begin{proposition}\label{CLTprop}
     A sufficient condition for
     \begin{equation}
         \frac{W_n}{\sqrt{\sum_{1\leq j<i\leq n} a_{ij}^2}}\xrightarrow{\mathcal{L}} N(0,1)
     \end{equation}
     is that
     \begin{equation}
         \sum_{j<k}{(\sum_{i:i>k}a_{ij}a_{ik})}^2+
         \sum_{j<i}a_{ij}^4+
         \sum_{j<k<i}a_{ij}^2 a_{ik}^2
         =o\big({(\sum_{j<i} a_{ij}^2)}^2\big).
     \end{equation}
 \end{proposition}

 \begin{proof}
     Define $U_{in} =\epsilon_i \sum_{j=1}^{i-1} a_{ij}\epsilon_j$, $i=2,\ldots,n$, and $\mathcal{F}_{in}=\sigma\{\epsilon_1,\ldots,\epsilon_i\}$, $i=1,\ldots, n$.
     Now $W_n=\sum_{i=2}^n U_{in}$, $\{U_{in}\}$ is a martingale difference array with respect to $\{\mathcal{F}_{in}\}$. 
     To prove the proposition, we shall verify two conditions (See~\cite{pollard1984convergence}):
     \begin{equation}\label{MCLTcondition1}
         \frac{\sum_{i=2}^n \mathrm{E}(U_{in}^2 |\mathcal{F}_{i-1,n})}{\sum_{1\leq j<i\leq n} a_{ij}^2}\xrightarrow{P} 1,
     \end{equation}
     and
     \begin{equation}\label{MCLTcondition2}
         \frac{\sum_{i=2}^n \mathrm{E}\big(U_{in}^2\big\{U_{in}^2>\epsilon \sum_{1\leq j<i\leq n} a_{ij}^2\big\}\big|\mathcal{F}_{i-1,n}\big)}{\sum_{1\leq j<i\leq n} a_{ij}^2}\xrightarrow{P} 0,
     \end{equation}
     for every $\epsilon>0$.

     \paragraph{Proof of~\eqref{MCLTcondition1}}
     Since $\mathrm{E}(U_{in}^2 |\mathcal{F}_{i-1,n})={(\sum_{j=1}^{i-1}a_{ij}\epsilon_j)}^2$, we have
     \begin{equation*}
         \begin{aligned}
\sum_{i=2}^n \mathrm{E}(U_{in}^2 |\mathcal{F}_{i-1,n})
             &=\sum_{i=2}^n \big(\sum_{j=1}^{i-1}a_{ij}\epsilon_j \big)^2
             =\sum_{i=2}^n \big( \sum_{j=1}^{i-1} a_{ij}^2 +2\sum_{j,k:j<k<i} a_{ij}a_{ik}\epsilon_j \epsilon_k \big)\\
             &=\sum_{i=2}^n  \sum_{j=1}^{i-1} a_{ij}^2 +2\sum_{j<k<i} a_{ij}a_{ik}\epsilon_j \epsilon_k.
         \end{aligned}
     \end{equation*}

     But
     \begin{equation*}
         \begin{aligned}
         \mathrm{E}{(\sum_{j<k<i} a_{ij}a_{ik}\epsilon_j \epsilon_k)}^2
             &=
             \mathrm{E}{\big(\sum_{j<k} (\sum_{i:i>k}a_{ij}a_{ik})\epsilon_j \epsilon_k \big)}^2\\
             &=
             \sum_{j<k} (\sum_{i:i>k}a_{ij}a_{ik})^2
             =
             o\big({(\sum_{j<i} a_{ij}^2)}^2\big),
         \end{aligned}
     \end{equation*}
     where the last equality holds by assumption. Hence~\eqref{MCLTcondition1} holds.
     \paragraph{Proof of~\eqref{MCLTcondition2}}
     By Markov's inequality, we only need to prove
     \begin{equation}\label{temp1}
         \frac{\sum_{i=2}^n \mathrm{E}\big(U_{in}^4\big|\mathcal{F}_{i-1,n}\big)}{{\big(\sum_{1\leq j<i\leq n} a_{ij}^2\big)}^2}\xrightarrow{P} 0.
     \end{equation}
     Since the relavant random variables are all positive, we only need to prove~\eqref{temp1} converges to $0$ in mean. But
     \begin{equation*}
         \begin{aligned}
         \sum_{i=2}^n \mathrm{E} U_{in}^4
             &=
             \sum_{i=2}^n \mathrm{E} {(\sum_{j:j<i}a_{ij}\epsilon_j)}^4
             =
             \sum_{i=2}^n \mathrm{E} {(\sum_{j:j<i}a_{ij}^2+2\sum_{j,k:j<k<i}a_{ij}a_{ik}\epsilon_j \epsilon_k)}^2\\
             &=
             \sum_{i=2}^n  \big({(\sum_{j:j<i}a_{ij}^2)}^2+4\mathrm{E}{(\sum_{j,k:j<k<i}a_{ij}a_{ik}\epsilon_j \epsilon_k)}^2 \big)\\
             &=
             \sum_{i=2}^n  (\sum_{j:j<i}a_{ij}^4+6\sum_{j,k:j<k<i}a_{ij}^2 a_{ik}^2)\\
             &=
             \sum_{j<i}a_{ij}^4+6\sum_{j<k<i}a_{ij}^2 a_{ik}^2
             =
             o\big({(\sum_{j<i} a_{ij}^2)}^2\big),
         \end{aligned}
     \end{equation*}
     where the last equality holds by assumption. Hence~\eqref{MCLTcondition2} holds.
 \end{proof}

\section{Asymptotic normality}
\begin{lemma}\label{lemmaUniformSimple}
    Suppose $\{\eta_n\}$ is a sequence of $1$-dimensional random variables, weakly converges to $\eta$, a random variable with continuous distribution function.
    Then we have
    $$
    \sup_{x}|\Pr(\eta_n\leq x)-\Pr(\eta\leq x)|\to 0.
    $$
\end{lemma}

\begin{lemma}\label{lemmaQ}
    Suppose $A=(a_{ij})$ is an $m\times m$ positive semi-definite matrix. Under~\eqref{chenC2}, we have
    \begin{equation}
        \mathrm{E} {(Z_i^T A Z_i)}^2\asymp {(\mathrm{tr}A)}^2
    \end{equation}
\begin{proof}
Notice that
\begin{equation}
    \begin{aligned}
        {(Z_i^T A Z_i)}^2
        =&
        (\sum_{j=1}^m a_{jj}z_{ij}^2+2\sum_{k<j}a_{jk}z_{ij}z_{ik})^2\\
        =&
        (\sum_{j=1}^m a_{jj}z_{ij}^2)^2+
        4(\sum_{j=1}^m a_{jj}z_{ij}^2)(\sum_{k<j}a_{jk}z_{ij}z_{ik})+
        4(\sum_{k<j}a_{jk}z_{ij}z_{ik})^2\\
        =&
        \sum_{j=1}^m a_{jj}^2z_{ij}^4+2\sum_{k<j}a_{jj}a_{kk}z_{ij}^2 z_{ik}^2+
        4(\sum_{j=1}^m a_{jj}z_{ij}^2)(\sum_{k<j}a_{jk}z_{ij}z_{ik})\\
        &+
        4(\sum_{k<j}a_{jk}^2z_{ij}^2z_{ik}^2+\sum_{k<j,l<\alpha:\mathrm{card}(\{k,j\}\cap\{l,\alpha\})<2} a_{jk}a_{\alpha l}z_{ij}z_{ik}z_{i\alpha}z_{il})\\
    \end{aligned}
\end{equation}
Hence
\begin{equation}
    \begin{aligned}
        \mathrm{E}{(Z_i^T A Z_i)}^2
        =&
        \sum_{j=1}^n a_{jj}^2 \mathrm{E}z_{ij}^4+2\sum_{k<j}a_{jj}a_{kk}\mathrm{E}(z_{ij}^2 z_{ik}^2)+
        4\sum_{k<j}a_{jk}^2 \mathrm{E}(z_{ij}^2z_{ik}^2)\\
        \asymp &
        \sum_{j=1}^n\sum_{k=1}^n a_{jj}a_{kk}+
        \sum_{j=1}^n\sum_{k=1}^n a_{jk}^2
        ={(\mathrm{tr}(A))}^2+\mathrm{tr}A^2.
    \end{aligned}
\end{equation}

    By Cauchy inequality, $0\leq \mathrm{tr} A^2\leq {(\mathrm{tr}A)}^2$. The conclusion holds.

\end{proof}
\end{lemma}

\begin{lemma}\label{smallLemma1}
    Under~\eqref{chenC1},~\eqref{chenC2}, for $i\neq j$ we have
    \begin{equation}\label{eq:20170220}
        \mathrm{E}{(X_i^T X_j)}^4=%O\Big({\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)}^2\Big).
             O(1){\Big(\mathrm{tr}{\big(\Sigma+\mu\mu^T\big)}^2\Big)}^2.
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            {(X_i^T X_j)}^4=&
            {(Z_i^T \Gamma^T \Gamma Z_j+\mu^T \Gamma Z_i+\mu^T \Gamma Z_j+\mu^T \mu)}^4\\
            \leq &
            64\big((Z_i^T \Gamma^T \Gamma Z_j)^4+(\mu^T \Gamma Z_i)^4+(\mu^T \Gamma Z_j)^4+(\mu^T \mu)^4\big)\\
        \end{aligned}
    \end{equation}
    We deal with the first term by applying Lemma~\ref{lemmaQ} twice.
    \begin{equation}
        \begin{aligned}
            \mathrm{E}(Z_i^T \Gamma^T \Gamma Z_j)^4=&
        \mathrm{E}(Z_i^T \Gamma^T \Gamma Z_j Z_j^T \Gamma^T \Gamma Z_i)^2
            =
            \mathrm{E}\mathrm{E}\big((Z_i^T \Gamma^T \Gamma Z_j Z_j^T \Gamma^T \Gamma Z_i)^2 | Z_j\big)\\
            \asymp &  \mathrm{E}{(Z_j^T \Gamma^T \Sigma \Gamma Z_j)}^2
            \asymp   {(\mathrm{tr}\Sigma^2)}^2,%+\mathrm{tr}\Sigma^4\\
            %\leq &  (\mathrm{tr}\Sigma^2)^2+\lambda_{\max}^2(\Sigma)\mathrm{tr}\Sigma^2,
        \end{aligned}
    \end{equation}
    Similarly, we have
    \begin{equation}
        \begin{aligned}
            &\mathrm{E}(\mu^T \Gamma Z_i)^4=
        \mathrm{E}(Z_i^T \Gamma^T \mu\mu^T \Gamma Z_i)^2
            \asymp  {(\mu^T \Sigma \mu)}^2\\
            &\leq \lambda_{\max}^2(\Sigma){(\mu^T \mu)}^2
        \leq \mathrm{tr}(\Sigma^2){(\mu^T \mu)}^2
            \leq {\big(\mathrm{tr}(\Sigma^2)\big)}^2+{(\mu^T \mu)}^4.
        %\leq \lambda_{\max}^2(\Sigma){(\mu^T\mu)}^2
        \end{aligned}
    \end{equation}
    Thus, the conclusion holds.
\end{proof}


\begin{lemma}\label{smallLemma2}
    Under~\eqref{chenC1},~\eqref{chenC2}, suppose $i\neq j$, $i\neq k$, $j\neq k$, we have
    \begin{equation}\label{eq:2}
            \mathrm{E}{(X_i^T X_j)}^2{(X_k^T X_i)}^2=
             O(1){\Big(\mathrm{tr}{\big(\Sigma+\mu\mu^T\big)}^2\Big)}^2.
    \end{equation}
\end{lemma}
\begin{proof}
Note that
    \begin{equation*}
        \begin{aligned}
            &\mathrm{E}{(X_i^T X_j)}^2{(X_k^T X_i)}^2
            = 
            \mathrm{E}\mathrm{E}\big({(X_i^T X_j)}^2{(X_k^T X_i)}^2| X_i\big)
            =
            \mathrm{E}{(X_i^T (\Sigma+\mu\mu^T) X_i )}^2\\
            =&
            \mathrm{E}{\big(Z_i^T \Gamma^T (\Sigma+\mu\mu^T) \Gamma Z_i+ 2\mu^T (\Sigma+\mu\mu^T)\Gamma Z_i +\mu \Sigma \mu +(\mu^T\mu)^2 \big)}^2\\
            \leq&
            4\mathrm{E}(Z_i^T \Gamma^T (\Sigma+\mu\mu^T) \Gamma Z_i)^2+ 16\mathrm{E}(\mu^T (\Sigma+\mu\mu^T)\Gamma Z_i)^2 +4(\mu \Sigma \mu)^2 +4(\mu^T\mu)^4.
        \end{aligned}
    \end{equation*}
    By Lemma~\eqref{lemmaQ},
    \begin{equation*}
        \begin{aligned}
\mathrm{E}(Z_i^T \Gamma^T (\Sigma+\mu\mu^T) \Gamma Z_i)^2
            &\asymp
\big(\mathrm{tr}(\Gamma^T (\Sigma+\mu\mu^T)\Gamma)\big)^2\\
            &=
\big(\mathrm{tr}\Sigma^2+\mu^T\Sigma \mu\big)^2
            \leq
            2{\big(\mathrm{tr}\Sigma^2\big)}^2+2{\big(\mu^T\Sigma \mu\big)}^2.
        \end{aligned}
    \end{equation*}
    And
    \begin{equation*}
        \begin{aligned}
            &\mathrm{E}(\mu^T (\Sigma+\mu\mu^T)\Gamma Z_i)^2=
\mu^T (\Sigma+\mu\mu^T)\Sigma (\Sigma+\mu\mu^T)\mu\\
            =&\mu^T \Sigma^3 \mu+2(\mu^T\mu)(\mu^T\Sigma^2\mu)+{(\mu^T\mu)}^2(\mu^T\Sigma\mu).
        \end{aligned}
    \end{equation*}

    But for $i=1,2,\ldots$, we have
    $$
    \mu^T \Sigma^i \mu
    \leq \lambda_{\max}^i(\Sigma)\mu^T\mu
    \leq {(\mathrm{tr}(\Sigma^2))}^{i/2}\mu^T\mu.
    $$
    Thus,
    \begin{equation}
        \begin{aligned}
            &\mathrm{E}{(X_i^T X_j)}^2{(X_k^T X_i)}^2\\
            =&
            O(1)\big({(\mathrm{tr}\Sigma^2)}^2+
            {(\mathrm{tr}\Sigma^2)}^{3/2}{(\mu^T \mu)}+
            {(\mathrm{tr}\Sigma^2)}{(\mu^T \mu)}^2+
            {(\mathrm{tr}\Sigma^2)}^{1/2}{(\mu^T \mu)}^3+
            {(\mu^T \mu)}^4
            \big)\\
            =& O(1){\Big(\mathrm{tr}{\big(\Sigma+\mu\mu^T\big)}^2\Big)}^2.
        \end{aligned}
    \end{equation}
\end{proof}


\begin{lemma}\label{ratioLemma}
    Under~\eqref{chenC1} and~\eqref{chenC2}, we have
    \begin{equation}
        \frac{\sum_{j<i}{(X_i^T X_j)}^2}{\frac{n(n-1)}{2}\mathrm{tr} (\Sigma+\mu\mu^T)^2}
        \xrightarrow{P}1.
    \end{equation}
\end{lemma}
\begin{proof}
    Since
    \begin{equation}
        \begin{aligned}
            \mathrm{E}{(X_i^T X_j)}^2=&
            \mathrm{E}(X_i^T X_j X_j^T X_i)=
            \mathrm{E}(X_i^T (\Sigma+\mu \mu^T) X_i)\\
            =&
            \mathrm{E}\mathrm{tr}((\Sigma+\mu \mu^T) X_i X_i^T)=\mathrm{tr}{(\Sigma+\mu \mu^T)}^2,
        \end{aligned}
    \end{equation}
   we have 
    \begin{equation}
        \mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2=\frac{n(n-1)}{2}\mathrm{tr}{(\Sigma+\mu\mu^T)}^2
    \end{equation}
    So we only need to consider the variance. According to $\mathrm{card}(\{i,j\}\cap\{k,l\})=0,1,2$, we have
    \begin{equation}\label{eq:1}
    \begin{aligned}
        &{\big(\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
        =
        \sum_{j<i}{(X_i^T X_j)}^4+
        \sum_{j<i,k<l:\{i,j\}\cap \{k,l\}=\phi}{(X_i^T X_j)}^2{(X_k^T X_l)}^2\\
        &+2\sum_{j<i<k}\big(
        {(X_i^T X_j)}^2{(X_k^T X_i)}^2+
{(X_i^T X_j)}^2{(X_k^T X_j)}^2+
{(X_k^T X_j)}^2{(X_k^T X_i)}^2
        \big).
    \end{aligned}
    \end{equation}


   
    In~\eqref{eq:1}, there are $n(n-1)/2$, $n(n-1)(n-2)(n-3)/4$ and $n(n-1)(n-2)/6$ terms in each summation. By Lemma~\ref{smallLemma1} and Lemma~\ref{smallLemma2}, we have
    \begin{equation}
    \begin{aligned}
        \mathrm{E}{\big(\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
            =&\frac{n(n-1)(n-2)(n-3)}{4}{\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)}^2\\
            &+O(1)(\frac{n(n-1)}{2}+n(n-1)(n-2)){\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)}^2.
    \end{aligned}
    \end{equation}
Hence 
    $$
    \frac{
        \mathrm{Var}(\sum_{j<i}{(X_i^T X_j)}^2)
    }{\big(\mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2\big)^2}
    =
    \frac{
        \mathrm{E}{\big(\sum_{j<i}{(X_i^T X_j)}^2\big)}^2-
        {\big(\mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
    }{
        {\big(\mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
    }
    =O(\frac{1}{n}).
    $$
    Thus the conclusion holds.
\end{proof}

\begin{lemma}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and 
    \begin{equation}\label{myLoc}
        \mu^T \mu=o(\sqrt{\mathrm{tr}\Sigma^2}),
    \end{equation}
we have
    \begin{equation}\label{lemma2R1}
        \sum_{j<k}{(\sum_{i:i>k}X_i^T X_j X_i^T X_k)}^2
        =o_P\Big(\big(\frac{n(n-1)}{2}\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2\Big).
    \end{equation}
    \begin{equation}\label{lemma2R2}
        \sum_{j<k}{(X_i^T X_j)}^4=o_P\Big(\big(\frac{n(n-1)}{2}\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2\Big)
    \end{equation}
    \begin{equation}\label{lemma2R3}
        \sum_{j<k<i}{(X_i^T X_j)}^2{(X_i^T X_k)}^2 =o_P\Big(\big(\frac{n(n-1)}{2}\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2\Big)
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{equation*}
    \begin{aligned}
        &\mathrm{E}\sum_{j<k}{(\sum_{i:i>k}X_i^T X_j X_i^T X_k)}^2\\
        =&
        \mathrm{E}\sum_{j<k}\Big(\sum_{i:i>k}(X_i^T X_j)^2 (X_i^T X_k)^2+2\sum_{i_1,i_2:i_1>i_2>k}X_{i_1}^T X_j X_{i_1}^T X_k X_{i_2}^T X_j X_{i_2}^T X_k\Big).\\
    \end{aligned}
    \end{equation*}

    By Lemma~\ref{smallLemma1}, we have
    \begin{equation*}
    \begin{aligned}
        \mathrm{E}\sum_{j<k<i}(X_i^T X_j)^2 (X_i^T X_k)^2    =O(n^3)\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2.
    \end{aligned}
    \end{equation*}
And
    \begin{equation*}
    \begin{aligned}
        &\mathrm{E}\sum_{j<k<i_2<i_1}X_{i_1}^T X_j X_{i_1}^T X_k X_{i_2}^T X_j X_{i_2}^T X_k\\
        =&\frac{n(n-1)(n-2)(n-3)}{6}\mathrm{tr}{(\Sigma+\mu\mu^T)}^4\\
        \leq& \frac{n(n-1)(n-2)(n-3)}{6}8(\mathrm{tr}{(\Sigma)}^4+(\mu^T \mu)^4)\\
        \leq& O(n^4)(\lambda_{\max}^2(\Sigma)\mathrm{tr}{(\Sigma)}^2+(\mu^T \mu)^4)\\
        =&
        o\Big(n^4{\big(\mathrm{tr}\Sigma^2\big)}^2\Big),
    \end{aligned}
    \end{equation*}
    where the last line follows by assumption~\eqref{chenC3} and~\eqref{myLoc}.
    Thus~\eqref{lemma2R1} holds.~\eqref{lemma2R2} and~\eqref{lemma2R3} follow by Lemma~\ref{smallLemma1} and Lemma~\ref{smallLemma2}.

\end{proof}

\begin{proof}[Proof of Theorem~\ref{shaziCLT}]
By a standard subsequence argument, we only need to prove 
    \begin{equation}\label{changchuan}
        \rho\Big(\mathcal{L}\Big(\frac{T_2(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big),N(0,1)\Big)\xrightarrow{a.s.} 0
    \end{equation}
     along a subsequence.
    But there is a subsequence $\{n(k)\}$ along which~\eqref{lemma2R1},~\eqref{lemma2R2} and~\eqref{lemma2R3} holds almost surely.
    By Proposition~\ref{CLTprop}, we have
    \begin{equation*}
        \mathcal{L}\Big(\frac{T_2(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big)\xrightarrow{\mathcal{L}}N(0,1)
    \end{equation*}
    almost surely along $\{n(k)\}$, which means~\eqref{changchuan} holds along $\{n(k)\}$.

\end{proof}



\begin{proof}[Proof of Theorem~\ref{theoremPower}]
    Note that
    \begin{align}
            &\Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\nonumber\\
            =&
            \Pr\Big(\frac{T( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^*-\frac{\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}} \Big)
            \label{powerEq1}
    \end{align}
    If~\eqref{mu2} holds, by Lemma~\ref{ratioLemma}, we have
    $$
    \frac{\sum_{1\leq j< i\leq n}(X_i^T X_j)^2}{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}\xrightarrow{P}1.
    $$
    By Corollary~\ref{corollaryQuan}, we have $\xi_{\alpha}^*\xrightarrow{P} \Phi(1-\alpha)$.
Thus,
    \begin{equation*}
        \begin{aligned}
            \eqref{powerEq1}=&
            \Pr\Big(\frac{T( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}-
            \frac{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}\xi_{\alpha}^*>
            -\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}} \Big)\\
            =&
            \Pr\Big(N(0,1)-\Phi(1-\alpha)>-\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}}\Big)+o(1)\\
            =&
            \Phi(-\Phi(1-\alpha)+\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}})+o(1),
        \end{aligned}
    \end{equation*}
    where the last two equality holds by Theorem~\ref{theoremChen}, Slutsky's theorem and Lemma~\ref{lemmaUniformSimple}.

    If~\eqref{mu3} holds, by Lemma~\ref{ratioLemma}, we have
    $$
    \frac{\sum_{1\leq j< i\leq n}(X_i^T X_j)^2}{\frac{n(n-1)}{2}{(\mu^T\mu)}^2}\xrightarrow{P}1.
    $$
    Thus
            
    \begin{equation*}
        \begin{aligned}
            &\frac{T( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}        \\
            =&\frac{T( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}
            \frac{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}{\sqrt{\frac{n(n-1)}{2}{(\mu^T \mu)}^2}}        
            \frac{\sqrt{\frac{n(n-1)}{2}{(\mu^T \mu)}^2}}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}        
            \xrightarrow{P} 0.
        \end{aligned}
    \end{equation*}
    Since $\xi_{\alpha}^*\xrightarrow{P}\frac{\sqrt{2}}{2}(\Phi^{-1}(1-\frac{\alpha}{2})-1)$ by Corollary~\ref{corollaryQuan2},  we have that $\eqref{powerEq1}\to 1$.


\end{proof}
\begin{proof}[Proof of Theorem~\ref{theoremPower2}]
    Note that
    \begin{align}
            &\Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\nonumber\\
            =&
            \Pr\Big(\frac{T( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{{(n-1)}^2 n \mu^T\Sigma\mu}}>
            \frac{\sqrt{\sum_{1\leq j<i\leq n}{{(X_i^T X_j)}^2}}}{\sqrt{{(n-1)}^2 n \mu^T\Sigma\mu}}\xi_{\alpha}^*-\frac{\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{{(n-1)}^2 n \mu^T\Sigma\mu}} \Big).
            \label{powerEq2}
    \end{align}
    If~\eqref{mu2} holds, the theorem follows by Theorem~\ref{theoremChen2} and the fact that if~\eqref{mu2} holds, the coefficient of $\xi_\alpha^*$ in~\eqref{powerEq2} tends to $0$.

    If~\eqref{mu3} holds, the theorem follows by noting that
    \begin{equation*}
        \begin{aligned}
            \eqref{powerEq2}=
            \Pr\Big(\frac{T( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{{(n-1)}^2 n \mu^T\Sigma\mu}}>
            -(1+o_P(1))\frac{\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{{(n-1)}^2 n \mu^T\Sigma\mu}} \Big).
        \end{aligned}
    \end{equation*}
\end{proof}

\begin{proof}[Proof of Theorem~\ref{farT}]
    \begin{equation*}
        \begin{aligned}
            \sum_{j<i} X_i^T X_j \epsilon_i\epsilon_j=&
            \sum_{j<i} Z_i^T \Gamma^T \Gamma Z_j \epsilon_i\epsilon_j\\
            &+
            \sum_{j<i} \mu^T \Gamma Z_i \epsilon_i\epsilon_j
            +\sum_{j<i} \mu^T \Gamma Z_j \epsilon_i\epsilon_j+
            \mu^T \mu \sum_{j<i} \epsilon_i\epsilon_j\\
            \overset{def}{=}&C_1+C_2+C_3+C_4.
        \end{aligned}
    \end{equation*}
Term $C_4$ plays a major role. Note that
    \begin{equation*}
        \begin{aligned}
            C_4=\frac{n}{2}\mu^T \mu\Big({\Big(\frac{1}{\sqrt{n}}\sum_{i=1}^n \epsilon_i\Big)}^2-1\Big).
        \end{aligned}
    \end{equation*}
By central limit theorem, we have
    \begin{equation*}
        \begin{aligned}
            \rho\Big(\mathcal{L}\Big(\frac{C_4}{\frac{n}{2}\mu^T \mu}\Big| X_1,\ldots,X_n\Big),\chi^2_1-1\Big)\xrightarrow{a.s.} 0.
        \end{aligned}
    \end{equation*}
Next we show that $C_1$, $C_2$ and $C_3$ are negligible under the assumptions of the theorem.
    By a standard subsequence argument and Slutsky's theorem, we can obtain
    \begin{equation}\label{rush2}
        \begin{aligned}
            \rho\Big(\mathcal{L}\Big(\frac{T(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\frac{n}{2}\mu^T \mu}\Big| X_1,\ldots,X_n\Big),\chi^2_1-1\Big)\xrightarrow{P} 0
        \end{aligned}
    \end{equation}
    by showing that
    $$
    \mathrm{E}\Big({\Big(\frac{C_i}{\frac{n}{2}\mu^T\mu}\Big)}^2\Big|X_1,\ldots,X_n\Big)\xrightarrow{P} 0,\quad i=1,2,3.
    $$
    It in turn suffices to show

    \begin{equation}\label{Rush}
    \mathrm{E}{\Big(\frac{C_i}{\frac{n}{2}\mu^T\mu}\Big)}^2\to 0,
    \quad i=1,2,3.
    \end{equation}
    By direct calculation, we have
    $$\mathrm{E}(C_1^2)=\mathrm{E}\mathrm{E}(C_1^2|X_1,\ldots,X_n)=\sum_{j<i}\mathrm{E}{(Z_i^T \Gamma^T \Gamma Z_j)}^2=\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2,$$
    and
    $$\mathrm{E}(C_2^2)=\mathrm{E}(C_3^2)=\frac{n(n-1)}{2}\mu^T \Sigma \mu\leq \frac{n(n-1)}{2}\sqrt{\mathrm{tr}\Sigma^2}\mu^T\mu.$$
    Thus~\eqref{Rush} follows by Assumption~\eqref{mu3}. Having~\eqref{rush2} holds, the theorem follows by Slutsky's theorem, Lemma~\ref{ratioLemma} and Assumption~\eqref{mu3}.
\end{proof}


\section*{References}

\bibliography{mybibfile}

\end{document}
