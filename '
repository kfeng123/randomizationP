\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}

\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xeCJK}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}




\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{Condition}

\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}




\begin{document}

\begin{frontmatter}

\title{a Randomization Test for Mean Vector In High Dimension}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
    Testing mean vector is a fundamental problem in statistics.
    On the one hand.
    Many test procedures for high dimensional mean vector have been proposed in the literature.
\end{abstract}

\begin{keyword}
\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers


\section{Introduction}

Consider i.i.d.\ random sample $X_{1},\ldots,X_{n}\in \mathbb{R}^p$ which has means $\mu={(\mu_1,\ldots,\mu_p)}^T$ and covariance matrix $\Sigma$. The one sample testing problem
\begin{equation}
    H_0:\mu=0_p\quad \textrm{versus} \quad H_1:\mu\neq 0_p
\end{equation}
has been extensively studied by many researchers.
A classical test statistic is Hotelling's $T^2$, 
    $
    T_{1}=n\bar{X}^T S^{-1}\bar{X}
    $,
where $\bar{X}$ and $S=(n-1)^{-1}\sum_{i=1}^n (X_i-\bar{X}) (X_i-\bar{X})^T$ are the sample mean vector and sample covariance matrix, respectively.
However, the high-dimensional case $d>n$ invalidates the Hotelling's test.
\cite{Bai1996Efiect} is the first paper modifying Hotelling's $T^2$ statistic for high dimensional situations.
This seminal paper removed $S^{-1}$ from $T_1$ and proposed a test statistic based on
$
T_2=\bar{X}^T\bar{X}
$.
Many subsequent papers relaxed the assymptions and generalized the idea of~\cite{Bai1996Efiect}.
For example,~\cite{Srivastava2009A} proposed a test based on
$
T_3=\bar{X}^T {[\mathrm{diag}(S)]}^{-1} \bar{X}
$,
where $\mathrm{diag}(S)$ is a matrix with diagonal elements equal to that of $S$ and off-diagonal elemnts equal to $0$.
\cite{Chen2010A} proposed a test based on
$
T_4=\sum_{i\neq j}X_i^T X_j
$.
Feng long proposed a test based on.

Note that these works are mostly can be written as a generalized quadratic form of data, see.
Their theoretical prove mostly rely on the application of martingale central limit theorem (MCLT) which is suitable for quadratic form.

The critical value of existing high dimensional tests are often determined by asymptotical distribution. Hence the test level can not be guaranteed in finite sample case.




The idea of randomization test dates back to~\cite{Fisher}, which is a tool to determine the critical value for a given test statistic.
It's strongth is in that the resulting test procedure has exact level under mild condition.
\cite{Romano1990On} described a general construction of the randomization test.
In this paper, we focus on a special randomization method for one sample mean testing problem, which we will introduce shortly.
If randomization method is robust and has good power in high dimension, it will be a better choice compared to asymptotic mehtod.
As far as we know, there's no existing theoretical work investigating the behavior of randomization test in high dimensional setting.

Suppose the distribution of $X_i$ is symmetric about $0$ under the null hypothesis. $T$ is a certain test statistic.
Let $\epsilon_1,\ldots,\epsilon_n$ be i.i.d.\ Rademacher variables ($\Pr(\epsilon_i=1)=\Pr(\epsilon_i=-1)=1/2$) which are independent of the data.
The conditional distribution
\begin{equation}\label{ranDis}
    \mathcal{L}(T(\epsilon_1 X_1,\ldots,\epsilon_i X_i,\ldots,\epsilon_n X_n)|X_1,\ldots,X_n)
\end{equation}
is the uniform distribution on $2^n$ values.
The critical value of the randomization test is defined as the $1-\alpha$ quantile of the above distribution.
 More specifically, the test function equals to $1$ or $0$ if $T(X_1,\ldots, X_n)$ is greater or not greater than the critical value. The resulting test is a level $\alpha$ test.
 By some refinement of the test function when $T(X_1,\ldots,X_n)$ equals to the critical value, the test is exact, see~\cite{Romano1990On}.
Since such extreme case occurs with little probability $2^{-n}$, such refinement is often dropped in practice, which losses only a little power.

Equivalently, the randomization test can be implemented by $p$-value. Define 
\begin{equation*}
    p(X_1,\ldots, X_n)=
    \Pr(T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)\geq T( X_1,\ldots,X_n)|X_1,\ldots,X_n).
\end{equation*}
The randomization test rejects the null hypothesis if $p(X_1,\ldots, X_n)\leq \alpha$. 



 It's easy to see that the randomization version of Bai's test and Chen's test are both equivelent to the randomization test based on
\begin{equation}\label{Statistic}
    T(X_1,\ldots,X_n)=\sum_{j<i}X_i^T X_j,
\end{equation}
the test statistic this paper adopts.
Other quadratic based statistic can be studied by similar method.

Our results show that even if the null distribution is not symmetric, the randomization test is still asymptotically exact. 
The local asymptotic power is also given.



%Existing works of randomization test mainly focus on single variate case. A recent work of EunYi Chung consider multivariate case.


In low dimensional setting, it's well known that randomization test has much higher time complexity than asymptotic method, which  historically  hampered it's use. 
Surprisingly, in high dimension setting the randomization test can be implemented as efficiently as asymptotic method.


%There's no need to estimate the variance of the statistic.

Maybe the most used randomization method is the two sample permutation test.
As~\cite{Romano1990On} pointed out, the asymptotic property of randomization tests depends heavily on the particular problem and, the two sample case is quite distinct from the one sample case.
The method used in this paper can not deal with permutation test.

\section{Randomization test}
We assume, like chen qin and bai, the following multivariate model:
\begin{equation}\label{chenC1}
    X_i=\mu+\Gamma Z_i\quad \textrm{for}\, i=1,\ldots,n,
\end{equation}
where $\Gamma$ is a $p\times m$ matrix for some $m\geq p$ such that $\Gamma_i\Gamma_i^T=\Sigma_i$ and ${\{Z_{i}\}}_{i=1}^n$ are $m$-variate i.i.d.\ random vectors satisfying $\mathrm{E}(Z_i)=0$ and $\mathrm{Var}(Z_i)=I_m$, the $m\times m$ identity matrix. Write $Z_i={(z_{i1},\ldots,z_{im})}^T$, we assume $\mathrm{E}(z_{ij}^4)=3+\Delta<\infty$ and
\begin{equation}\label{chenC2}
    \mathrm{E}(z_{il_1}^{\alpha_1}z_{il_2}^{\alpha_2}\cdots z_{il_q}^{\alpha_q})=\mathrm{E}(z_{il_1}^{\alpha_1})\mathrm{E}(z_{il_2}^{\alpha_2})\cdots \mathrm{E}(z_{il_q}^{\alpha_q})
\end{equation}
for a positive integer $q$ such that $\sum_{l=1}^q \alpha_l\leq 8$ and $l_1\neq l_2\neq \cdots \neq l_q$.

In the following, $T(X_1,\ldots, X_n)$ will be specialized to~\eqref{Statistic}.

An important assumption in Chen and Qin's paper is
    $\mathrm{tr}(\Sigma^4)=o\{\mathrm{tr}^2(\Sigma^2)\}$,
which is equavelent to 
\begin{equation}\label{chenC3}
    \frac{\lambda_{\max}(\Sigma)}{\sqrt{\mathrm{tr}\Sigma^2}}\to 0.
\end{equation}
Although Chen and Qin's results is for two sample case, their results can be proved similarly for one sample case. We restate their theorems:
\begin{theorem}\label{theoremChen}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and local alternatives
    \begin{equation}\label{mu1}
        \mu^T \Sigma\mu=o(n^{-1}\mathrm{tr}(\Sigma)^2),
    \end{equation}
    we have
    \begin{equation}
        \frac{T(X_1,\ldots,X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}\xrightarrow{\mathcal{L}}N(0,1).
    \end{equation}
\end{theorem}
And
\begin{theorem}\label{theoremChen2}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and     \begin{equation}\label{mu1}
     n^{-1}\mathrm{tr}(\Sigma)^2   =o(\mu^T \Sigma\mu),
    \end{equation}
    we have
    \begin{equation}
        \frac{T(X_1,\ldots,X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{{(n-1)}^2 n \mu^T \Sigma\mu}}\xrightarrow{\mathcal{L}}N(0,1).
    \end{equation}
\end{theorem}

We will call the conditional distribution
\begin{equation*}
        \mathcal{L}\Big(\frac{T(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big)
\end{equation*}
the randomization distribution.
Let $\xi^*_{\alpha}$ be the $1-\alpha$ quantile of the randomization distribution.
Then the test function $\phi(X_1,\ldots,X_n)$ equals to $1$ when 
$$
\frac{T(X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}> \xi^*_{\alpha}
$$
and equals to $0$ otherwise.
Since $\xi^*_{\alpha}$ relies on data, the rejection region is determined by not only  $T(X_1,\ldots,X_n)$ but also randomization distribution.

To study the asymptotic property of $\xi^*_{\alpha}$, we need to derive the asymptotic behavior of randomization distribution.
Since the randomization distribution itself is random, we need to define in what sence the convergence is. Let $F$ and $G$ be two distribution function on $\mathbb{R}$ and the Levy metric $\rho$ of $F$ and $G$ is defined as
\begin{equation*}
    \rho(F,G)=\inf\{\epsilon:F(x-\epsilon)-\epsilon\leq G(x)\leq F(x+\epsilon)+\epsilon\quad  \textrm{for all}\quad x\}.
\end{equation*}
It's well known that $\rho(F_n,F)\to 0$ if and only if  $F_n\xrightarrow{\mathcal{L}}F$.

Our asymptotic results need a condition stronger than~\eqref{mu1}.
\begin{theorem}\label{shaziCLT}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and local alternatives
    \begin{equation}\label{mu2}
        \mu^T\mu=o\big(\sqrt{\mathrm{tr}{(\Sigma)}^2}\big),
    \end{equation}
    we have that
    \begin{equation*}
        \rho\Big(\mathcal{L}\Big(\frac{T_2(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big),N(0,1)\Big)\xrightarrow{P} 0
    \end{equation*}
\end{theorem}
\begin{corollary}\label{corollaryQuan}
    Under the conditions of Theorem~\ref{shaziCLT}, we have $\xi_{\alpha}^*\xrightarrow{P} \Phi(1-\alpha)$, where $\Phi(\cdot)$ is the CDF of standard normal distribution.
\end{corollary}
\begin{remark}
    The proof of this theorem relies on Proposition~\ref{CLTprop}, a central limit theorem (CLT). 
\end{remark}
\begin{remark}
    Condition~\eqref{mu2} is stronger than~\eqref{mu1}, which suggests the power function of randomization test may be a little bit smaller than non-randomization test.
\end{remark}

We can derive the asymptotic power of randomization test.
\begin{theorem}\label{theoremPower}
    Under the conditions of Theorem~\ref{shaziCLT} and Theorem~\ref{theoremChen}, we have 
    \begin{equation*}
            \Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\\
            =
            \Phi(-\Phi^{-1}(1-\alpha)+\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}})+o(1),
    \end{equation*}
\end{theorem}
\begin{remark}
    The theorem doesn't assume the distribution of $X_i$ is symmetric under null. Hence randomization test is robust in this sense.
\end{remark}

When Assumption~\eqref{mu2} is not valid, we have following theorem
\begin{theorem}\label{farT}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and
    \begin{equation}\label{mu3}
       \sqrt{\mathrm{tr}{(\Sigma)}^2} =o\big(\mu^T\mu\big),
    \end{equation}
    we have
    \begin{equation}
        \rho\Big(\mathcal{L}\Big(\frac{T(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big),\frac{\sqrt{2}}{2}\big(\chi^2_1-1\big)\Big)\xrightarrow{P} 0
    \end{equation}
\end{theorem}
\begin{corollary}
    Under the conditions of Theorem~\ref{farT},
    $$\xi_{\alpha}^*\xrightarrow{P}\frac{\sqrt{2}}{2}(\Phi^{-1}(1-\frac{\alpha}{2})-1).$$
\end{corollary}
\begin{remark}
    When $\alpha=0.05$, $\Phi^{-1}(1-\alpha)\approx 1.645$ and $\frac{\sqrt{2}}{2}(\Phi^{-1}(1-\frac{\alpha}{2})-1)\approx 0.689$.
\end{remark}




\begin{theorem}\label{theoremPower2}
    Under the conditions of Theorem~\ref{shaziCLT} and Theorem~\ref{theoremChen2}, we have 
    \begin{equation*}
            \Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\\
            =
            \Phi(\frac{\sqrt{n}\mu^T\mu}{2\sqrt{\mu^T \Sigma \mu}})+o(1),
    \end{equation*}
\end{theorem}
\begin{remark}
Chen's test power is
    \begin{equation*}
            \Pr\Big(\frac{T( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\\
            =
            \Phi(-\Phi^{-1}(1-\alpha)+\frac{\sqrt{n}\mu^T\mu}{2\sqrt{\mu^T \Sigma \mu}})+o(1),
    \end{equation*}
    Hence our method is a bit more powerful.
\end{remark}


\section{Algorithm}

The quantile $\xi^*_\alpha$ might not be computationally feasible, since the randomized test statistic is (conditionally) uniformly distributed on $2^n$ different values.
In practice, randomization test is often realized through an approximation of $p$-value.
More specifically, we can sample from randomization distribution by generate $\epsilon_1,\ldots,\epsilon_n$ and compute $T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)$.
Repeat $M$ times for a large $M$ and we obtain $T_i^*$, $i=1,\ldots,M$.
%Denote $\xi_i=\mathbf{1}_{\{T_i^*\geq T_0\}}$.
Then  $\sum_{i=1}^M \mathbf{1}_{\{T_i^*\geq T_0\}}/M$ is an approximation of the $p$-value 
$$p(X_1,\ldots,X_n)=\Pr(T_i^*\geq T_0|X_1,\ldots,X_n).$$
Hence we reject the null if $\sum_{i=1}^M \mathbf{1}_{\{T_i^*\geq T_0\}}/M\leq \alpha$.

To be more efficient, we can accept the null once the sum of $\xi_i$ exceeds $M\alpha$ and reject the null once the sum of $1-\xi_i$ reaches $M(1-\alpha)$.

Note that once we have obtained $X_i^T X_j$, the computation of $T_i^*$ has only complexity $O(n^2)$.
The total complexity is thus $O(n^2 (p+M))$. Hence randomization test is very efficient in high dimension setting.
For example, when $M=p$, time spent by randomization is at most twice of that of asymptotic method.
This is different from low dimension setting where randomization is a lot slower than asymptotic method.



%We shall choose $M$ large enough such that 
%$$\Pr\Big(\Big|\frac{1}{M}\sum_{i=1}^M \xi-p(X_1,\ldots,X_n)\Big|>t|X_1,\ldots,X_n\Big)$$
%is smaller than $\epsilon$, where $t$ and $\epsilon$ are specified. By Hoeffding's inequality,
%\begin{equation*}
    %\Pr\Big(\Big|\frac{1}{M}\sum_{i=1}^M \xi-p(X_1,\ldots,X_n)\Big|>t|X_1,\ldots,X_n\Big)\leq 2e^{-2Mt^2}.
%\end{equation*}
%Hence we choose $M=\big[\frac{1}{2t^2}\log(\frac{2}{\epsilon})\big]+1$.






\begin{algorithm}
    \caption{Randomization Algorithm}
\label{theAlgorithm}
    \begin{algorithmic}
        \REQUIRE  $\alpha$, $M$
        \STATE Set $A\gets 0$, $T_0\gets T(X_1,\ldots,X_n)$.
        \STATE Compute $X_i^T X_j$ for $1\leq j< i\leq n$
        \FOR{$i=1$ to $M$}
            \STATE Generate $\epsilon_1,\ldots,\epsilon_n$ and compute $T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)=\sum_{j<i} X_i^T X_j\epsilon_i\epsilon_j$.
            \IF{$T(\epsilon_1 X_1,\ldots,\epsilon_n X_n)>T_0$}
            \STATE $A\gets A+1$
            \ENDIF
            \IF{$A> M\alpha$}
            \RETURN{Accept}
            \ENDIF
            \IF{$i-A\geq M(1-\alpha)$}
            \RETURN{Reject}
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}



\section{Appendix}
\paragraph{CLT for quadratic form of Rademacher varaibles}
Our proof of the main results is based on a CLT of the quadratic form of Rademacher variables. 
Such a CLT can be also used to study the asymptotic behavior of many other randomization test.
 Let $\epsilon_1,\ldots,\epsilon_n$ be indepent Rademacher  variables. 
 Consider quadratic form $W_n=\sum_{1\leq j<i\leq n} a_{ij}\epsilon_i \epsilon_j$, where $\{a_{ij}\}$ are nonrandom numbers. Here $\{\epsilon_i\}$ and $\{a_{ij}\}$ may depend on $n$, a parameter we suppress.
 Obviously, $\mathrm{E}(W_n)=0$ and $\mathrm{Var}(W_n)=\sum_{1\leq j<i\leq n} a_{ij}^2$.

 \begin{proposition}\label{CLTprop}
     A sufficient condition for
     \begin{equation}
         \frac{W_n}{\sqrt{\sum_{1\leq j<i\leq n} a_{ij}^2}}\xrightarrow{\mathcal{L}} N(0,1)
     \end{equation}
     is that
     \begin{equation}
         \sum_{j<k}{(\sum_{i:i>k}a_{ij}a_{ik})}^2+
         \sum_{j<i}a_{ij}^4+
         \sum_{j<k<i}a_{ij}^2 a_{ik}^2
         =o\big({(\sum_{j<i} a_{ij}^2)}^2\big).
     \end{equation}
 \end{proposition}

 \begin{proof}
     Define $U_{in} =\epsilon_i \sum_{j=1}^{i-1} a_{ij}\epsilon_j$, $i=2,\ldots,n$, and $\mathcal{F}_{in}=\sigma\{\epsilon_1,\ldots,\epsilon_i\}$, $i=1,\ldots, n$.
     Now $W_n=\sum_{i=2}^n U_{in}$, $\{U_{in}\}$ is a martingale difference array with respect to $\{\mathcal{F}_{in}\}$. 
     To prove the proposition, we shall verify two conditions (See David Pollard's book):
     \begin{equation}\label{MCLTcondition1}
         \frac{\sum_{i=2}^n \mathrm{E}(U_{in}^2 |\mathcal{F}_{i-1,n})}{\sum_{1\leq j<i\leq n} a_{ij}^2}\xrightarrow{P} 1,
     \end{equation}
     and
     \begin{equation}\label{MCLTcondition2}
         \frac{\sum_{i=2}^n \mathrm{E}\big(U_{in}^2\big\{U_{in}^2>\epsilon \sum_{1\leq j<i\leq n} a_{ij}^2\big\}\big|\mathcal{F}_{i-1,n}\big)}{\sum_{1\leq j<i\leq n} a_{ij}^2}\xrightarrow{P} 0,
     \end{equation}
     for every $\epsilon>0$.

     \paragraph{Proof of~\eqref{MCLTcondition1}}
     Since $\mathrm{E}(U_{in}^2 |\mathcal{F}_{i-1,n})={(\sum_{j=1}^{i-1}a_{ij}\epsilon_j)}^2$, we have
     \begin{equation*}
         \begin{aligned}
\sum_{i=2}^n \mathrm{E}(U_{in}^2 |\mathcal{F}_{i-1,n})
             &=\sum_{i=2}^n \big(\sum_{j=1}^{i-1}a_{ij}\epsilon_j \big)^2
             =\sum_{i=2}^n \big( \sum_{j=1}^{i-1} a_{ij}^2 +2\sum_{j,k:j<k<i} a_{ij}a_{ik}\epsilon_j \epsilon_k \big)\\
             &=\sum_{i=2}^n  \sum_{j=1}^{i-1} a_{ij}^2 +2\sum_{j<k<i} a_{ij}a_{ik}\epsilon_j \epsilon_k.
         \end{aligned}
     \end{equation*}

     But
     \begin{equation*}
         \begin{aligned}
         \mathrm{E}{(\sum_{j<k<i} a_{ij}a_{ik}\epsilon_j \epsilon_k)}^2
             &=
             \mathrm{E}{\big(\sum_{j<k} (\sum_{i:i>k}a_{ij}a_{ik})\epsilon_j \epsilon_k \big)}^2\\
             &=
             \sum_{j<k} (\sum_{i:i>k}a_{ij}a_{ik})^2
             =
             o\big({(\sum_{j<i} a_{ij}^2)}^2\big),
         \end{aligned}
     \end{equation*}
     where the last equality holds by assumption. Hence~\eqref{MCLTcondition1} holds.
     \paragraph{Proof of~\eqref{MCLTcondition2}}
     By Markov's inequality, we only need to prove
     \begin{equation}\label{temp1}
         \frac{\sum_{i=2}^n \mathrm{E}\big(U_{in}^4\big|\mathcal{F}_{i-1,n}\big)}{{\big(\sum_{1\leq j<i\leq n} a_{ij}^2\big)}^2}\xrightarrow{P} 0.
     \end{equation}
     Since the relavant random variables are all positive, we only need to prove~\eqref{temp1} converges to $0$ in mean. But
     \begin{equation*}
         \begin{aligned}
         \sum_{i=2}^n \mathrm{E} U_{in}^4
             &=
             \sum_{i=2}^n \mathrm{E} {(\sum_{j:j<i}a_{ij}\epsilon_j)}^4
             =
             \sum_{i=2}^n \mathrm{E} {(\sum_{j:j<i}a_{ij}^2+2\sum_{j,k:j<k<i}a_{ij}a_{ik}\epsilon_j \epsilon_k)}^2\\
             &=
             \sum_{i=2}^n  \big({(\sum_{j:j<i}a_{ij}^2)}^2+4\mathrm{E}{(\sum_{j,k:j<k<i}a_{ij}a_{ik}\epsilon_j \epsilon_k)}^2 \big)\\
             &=
             \sum_{i=2}^n  (\sum_{j:j<i}a_{ij}^4+6\sum_{j,k:j<k<i}a_{ij}^2 a_{ik}^2)\\
             &=
             \sum_{j<i}a_{ij}^4+6\sum_{j<k<i}a_{ij}^2 a_{ik}^2
             =
             o\big({(\sum_{j<i} a_{ij}^2)}^2\big),
         \end{aligned}
     \end{equation*}
     where the last equality holds by assumption. Hence~\eqref{MCLTcondition2} holds.
 \end{proof}

\section{Asymptotic normality}
\begin{lemma}\label{lemmaUniformSimple}
    Suppose $\{\eta_n\}$ is a sequence of $1$-dimensional random variables, weakly converges to $\eta$, a random variable with continuous distribution function.
    Then we have
    $$
    \sup_{x}|\Pr(\eta_n\leq x)-\Pr(\eta\leq x)|\to 0.
    $$
\end{lemma}

\begin{lemma}\label{lemmaQ}
    Suppose $A=(a_{ij})$ is an $m\times m$ positive semi-definite matrix. Under~\eqref{chenC2}, we have
    \begin{equation}
        \mathrm{E} {(Z_i^T A Z_i)}^2\asymp {(\mathrm{tr}A)}^2
    \end{equation}
\begin{proof}
Notice that
\begin{equation}
    \begin{aligned}
        {(Z_i^T A Z_i)}^2
        =&
        (\sum_{j=1}^m a_{jj}z_{ij}^2+2\sum_{k<j}a_{jk}z_{ij}z_{ik})^2\\
        =&
        (\sum_{j=1}^m a_{jj}z_{ij}^2)^2+
        4(\sum_{j=1}^m a_{jj}z_{ij}^2)(\sum_{k<j}a_{jk}z_{ij}z_{ik})+
        4(\sum_{k<j}a_{jk}z_{ij}z_{ik})^2\\
        =&
        \sum_{j=1}^m a_{jj}^2z_{ij}^4+2\sum_{k<j}a_{jj}a_{kk}z_{ij}^2 z_{ik}^2+
        4(\sum_{j=1}^m a_{jj}z_{ij}^2)(\sum_{k<j}a_{jk}z_{ij}z_{ik})\\
        &+
        4(\sum_{k<j}a_{jk}^2z_{ij}^2z_{ik}^2+\sum_{k<j,l<\alpha:\mathrm{card}(\{k,j\}\cap\{l,\alpha\})<2} a_{jk}a_{\alpha l}z_{ij}z_{ik}z_{i\alpha}z_{il})\\
    \end{aligned}
\end{equation}
Hence
\begin{equation}
    \begin{aligned}
        \mathrm{E}{(Z_i^T A Z_i)}^2
        =&
        \sum_{j=1}^n a_{jj}^2 \mathrm{E}z_{ij}^4+2\sum_{k<j}a_{jj}a_{kk}\mathrm{E}(z_{ij}^2 z_{ik}^2)+
        4\sum_{k<j}a_{jk}^2 \mathrm{E}(z_{ij}^2z_{ik}^2)\\
        \asymp &
        \sum_{j=1}^n\sum_{k=1}^n a_{jj}a_{kk}+
        \sum_{j=1}^n\sum_{k=1}^n a_{jk}^2
        ={(\mathrm{tr}(A))}^2+\mathrm{tr}A^2.
    \end{aligned}
\end{equation}

    By Cauchy inequality, $0\leq \mathrm{tr} A^2\leq {(\mathrm{tr}A)}^2$. The conclusion holds.

\end{proof}
\end{lemma}

\begin{lemma}\label{smallLemma1}
    Under~\eqref{chenC1},~\eqref{chenC2}, for $i\neq j$ we have
    \begin{equation}\label{eq:20170220}
        \mathrm{E}{(X_i^T X_j)}^4=%O\Big({\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)}^2\Big).
             O(1){\Big(\mathrm{tr}{\big(\Sigma+\mu\mu^T\big)}^2\Big)}^2.
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            {(X_i^T X_j)}^4=&
            {(Z_i^T \Gamma^T \Gamma Z_j+\mu^T \Gamma Z_i+\mu^T \Gamma Z_j+\mu^T \mu)}^4\\
            \leq &
            64\big((Z_i^T \Gamma^T \Gamma Z_j)^4+(\mu^T \Gamma Z_i)^4+(\mu^T \Gamma Z_j)^4+(\mu^T \mu)^4\big)\\
        \end{aligned}
    \end{equation}
    We deal with the first term by applying Lemma~\ref{lemmaQ} twice.
    \begin{equation}
        \begin{aligned}
            \mathrm{E}(Z_i^T \Gamma^T \Gamma Z_j)^4=&
        \mathrm{E}(Z_i^T \Gamma^T \Gamma Z_j Z_j^T \Gamma^T \Gamma Z_i)^2
            =
            \mathrm{E}\mathrm{E}\big((Z_i^T \Gamma^T \Gamma Z_j Z_j^T \Gamma^T \Gamma Z_i)^2 | Z_j\big)\\
            \asymp &  \mathrm{E}{(Z_j^T \Gamma^T \Sigma \Gamma Z_j)}^2
            \asymp   {(\mathrm{tr}\Sigma^2)}^2,%+\mathrm{tr}\Sigma^4\\
            %\leq &  (\mathrm{tr}\Sigma^2)^2+\lambda_{\max}^2(\Sigma)\mathrm{tr}\Sigma^2,
        \end{aligned}
    \end{equation}
    Similarly, we have
    \begin{equation}
        \begin{aligned}
            &\mathrm{E}(\mu^T \Gamma Z_i)^4=
        \mathrm{E}(Z_i^T \Gamma^T \mu\mu^T \Gamma Z_i)^2
            \asymp  {(\mu^T \Sigma \mu)}^2\\
            &\leq \lambda_{\max}^2(\Sigma){(\mu^T \mu)}^2
        \leq \mathrm{tr}(\Sigma^2){(\mu^T \mu)}^2
            \leq {\big(\mathrm{tr}(\Sigma^2)\big)}^2+{(\mu^T \mu)}^4.
        %\leq \lambda_{\max}^2(\Sigma){(\mu^T\mu)}^2
        \end{aligned}
    \end{equation}
    Thus, the conclusion holds.
\end{proof}


\begin{lemma}\label{smallLemma2}
    Under~\eqref{chenC1},~\eqref{chenC2}, suppose $i\neq j$, $i\neq k$, $j\neq k$, we have
    \begin{equation}\label{eq:2}
            \mathrm{E}{(X_i^T X_j)}^2{(X_k^T X_i)}^2=
             O(1){\Big(\mathrm{tr}{\big(\Sigma+\mu\mu^T\big)}^2\Big)}^2.
    \end{equation}
\end{lemma}
\begin{proof}
Note that
    \begin{equation*}
        \begin{aligned}
            &\mathrm{E}{(X_i^T X_j)}^2{(X_k^T X_i)}^2
            = 
            \mathrm{E}\mathrm{E}\big({(X_i^T X_j)}^2{(X_k^T X_i)}^2| X_i\big)
            =
            \mathrm{E}{(X_i^T (\Sigma+\mu\mu^T) X_i )}^2\\
            =&
            \mathrm{E}{\big(Z_i^T \Gamma^T (\Sigma+\mu\mu^T) \Gamma Z_i+ 2\mu^T (\Sigma+\mu\mu^T)\Gamma Z_i +\mu \Sigma \mu +(\mu^T\mu)^2 \big)}^2\\
            \leq&
            4\mathrm{E}(Z_i^T \Gamma^T (\Sigma+\mu\mu^T) \Gamma Z_i)^2+ 16\mathrm{E}(\mu^T (\Sigma+\mu\mu^T)\Gamma Z_i)^2 +4(\mu \Sigma \mu)^2 +4(\mu^T\mu)^4.
        \end{aligned}
    \end{equation*}
    By Lemma~\eqref{lemmaQ},
    \begin{equation*}
        \begin{aligned}
\mathrm{E}(Z_i^T \Gamma^T (\Sigma+\mu\mu^T) \Gamma Z_i)^2
            &\asymp
\big(\mathrm{tr}(\Gamma^T (\Sigma+\mu\mu^T)\Gamma)\big)^2\\
            &=
\big(\mathrm{tr}\Sigma^2+\mu^T\Sigma \mu\big)^2
            \leq
            2{\big(\mathrm{tr}\Sigma^2\big)}^2+2{\big(\mu^T\Sigma \mu\big)}^2.
        \end{aligned}
    \end{equation*}
    And
    \begin{equation*}
        \begin{aligned}
            &\mathrm{E}(\mu^T (\Sigma+\mu\mu^T)\Gamma Z_i)^2=
\mu^T (\Sigma+\mu\mu^T)\Sigma (\Sigma+\mu\mu^T)\mu\\
            =&\mu^T \Sigma^3 \mu+2(\mu^T\mu)(\mu^T\Sigma^2\mu)+{(\mu^T\mu)}^2(\mu^T\Sigma\mu).
        \end{aligned}
    \end{equation*}

    But for $i=1,2,\ldots$, we have
    $$
    \mu^T \Sigma^i \mu
    \leq \lambda_{\max}^i(\Sigma)\mu^T\mu
    \leq {(\mathrm{tr}(\Sigma^2))}^{i/2}\mu^T\mu.
    $$
    Thus,
    \begin{equation}
        \begin{aligned}
            &\mathrm{E}{(X_i^T X_j)}^2{(X_k^T X_i)}^2\\
            =&
            O(1)\big({(\mathrm{tr}\Sigma^2)}^2+
            {(\mathrm{tr}\Sigma^2)}^{3/2}{(\mu^T \mu)}+
            {(\mathrm{tr}\Sigma^2)}{(\mu^T \mu)}^2+
            {(\mathrm{tr}\Sigma^2)}^{1/2}{(\mu^T \mu)}^3+
            {(\mu^T \mu)}^4
            \big)\\
            =& O(1){\Big(\mathrm{tr}{\big(\Sigma+\mu\mu^T\big)}^2\Big)}^2.
        \end{aligned}
    \end{equation}
\end{proof}


\begin{lemma}\label{ratioLemma}
    Under~\eqref{chenC1} and~\eqref{chenC2}, we have
    \begin{equation}
        \frac{\sum_{j<i}{(X_i^T X_j)}^2}{\frac{n(n-1)}{2}\mathrm{tr} (\Sigma+\mu\mu^T)^2}
        \xrightarrow{P}1.
    \end{equation}
\end{lemma}
\begin{proof}
    Since
    \begin{equation}
        \begin{aligned}
            \mathrm{E}{(X_i^T X_j)}^2=&
            \mathrm{E}(X_i^T X_j X_j^T X_i)=
            \mathrm{E}(X_i^T (\Sigma+\mu \mu^T) X_i)\\
            =&
            \mathrm{E}\mathrm{tr}((\Sigma+\mu \mu^T) X_i X_i^T)=\mathrm{tr}{(\Sigma+\mu \mu^T)}^2,
        \end{aligned}
    \end{equation}
   we have 
    \begin{equation}
        \mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2=\frac{n(n-1)}{2}\mathrm{tr}{(\Sigma+\mu\mu^T)}^2
    \end{equation}
    So we only need to consider the variance. According to $\mathrm{card}(\{i,j\}\cap\{k,l\})=0,1,2$, we have
    \begin{equation}\label{eq:1}
    \begin{aligned}
        &{\big(\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
        =
        \sum_{j<i}{(X_i^T X_j)}^4+
        \sum_{j<i,k<l:\{i,j\}\cap \{k,l\}=\phi}{(X_i^T X_j)}^2{(X_k^T X_l)}^2\\
        &+2\sum_{j<i<k}\big(
        {(X_i^T X_j)}^2{(X_k^T X_i)}^2+
{(X_i^T X_j)}^2{(X_k^T X_j)}^2+
{(X_k^T X_j)}^2{(X_k^T X_i)}^2
        \big).
    \end{aligned}
    \end{equation}


   
    In~\eqref{eq:1}, there are $n(n-1)/2$, $n(n-1)(n-2)(n-3)/4$ and $n(n-1)(n-2)/6$ terms in each summation. By Lemma~\ref{smallLemma1} and Lemma~\ref{smallLemma2}, we have
    \begin{equation}
    \begin{aligned}
        \mathrm{E}{\big(\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
            =&\frac{n(n-1)(n-2)(n-3)}{4}{\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)}^2\\
            &+O(1)(\frac{n(n-1)}{2}+n(n-1)(n-2)){\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)}^2.
    \end{aligned}
    \end{equation}
Hence 
    $$
    \frac{
        \mathrm{Var}(\sum_{j<i}{(X_i^T X_j)}^2)
    }{\big(\mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2\big)^2}
    =
    \frac{
        \mathrm{E}{\big(\sum_{j<i}{(X_i^T X_j)}^2\big)}^2-
        {\big(\mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
    }{
        {\big(\mathrm{E}\sum_{j<i}{(X_i^T X_j)}^2\big)}^2
    }
    =O(\frac{1}{n}).
    $$
    Thus the conclusion holds.
\end{proof}

\begin{lemma}
    Under~\eqref{chenC1},~\eqref{chenC2},~\eqref{chenC3} and local alternatives
    \begin{equation}\label{myLoc}
        \mu^T \mu=o(\sqrt{\mathrm{tr}\Sigma^2}),
    \end{equation}
we have
    \begin{equation}\label{lemma2R1}
        \sum_{j<k}{(\sum_{i:i>k}X_i^T X_j X_i^T X_k)}^2
        =o_P\Big(\big(\frac{n(n-1)}{2}\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2\Big).
    \end{equation}
    \begin{equation}\label{lemma2R2}
        \sum_{j<k}{(X_i^T X_j)}^4=o_P\Big(\big(\frac{n(n-1)}{2}\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2\Big)
    \end{equation}
    \begin{equation}\label{lemma2R3}
        \sum_{j<k<i}{(X_i^T X_j)}^2{(X_i^T X_k)}^2 =o_P\Big(\big(\frac{n(n-1)}{2}\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2\Big)
    \end{equation}
\end{lemma}
\begin{proof}
    \begin{equation*}
    \begin{aligned}
        &\mathrm{E}\sum_{j<k}{(\sum_{i:i>k}X_i^T X_j X_i^T X_k)}^2\\
        =&
        \mathrm{E}\sum_{j<k}\Big(\sum_{i:i>k}(X_i^T X_j)^2 (X_i^T X_k)^2+2\sum_{i_1,i_2:i_1>i_2>k}X_{i_1}^T X_j X_{i_1}^T X_k X_{i_2}^T X_j X_{i_2}^T X_k\Big).\\
    \end{aligned}
    \end{equation*}

    By Lemma~\ref{smallLemma1}, we have
    \begin{equation*}
    \begin{aligned}
        \mathrm{E}\sum_{j<k<i}(X_i^T X_j)^2 (X_i^T X_k)^2    =O(n^3)\big(\mathrm{tr}(\Sigma+\mu\mu^T)^2\big)^2.
    \end{aligned}
    \end{equation*}
And
    \begin{equation*}
    \begin{aligned}
        &\mathrm{E}\sum_{j<k<i_2<i_1}X_{i_1}^T X_j X_{i_1}^T X_k X_{i_2}^T X_j X_{i_2}^T X_k\\
        =&\frac{n(n-1)(n-2)(n-3)}{6}\mathrm{tr}{(\Sigma+\mu\mu^T)}^4\\
        \leq& \frac{n(n-1)(n-2)(n-3)}{6}8(\mathrm{tr}{(\Sigma)}^4+(\mu^T \mu)^4)\\
        \leq& O(n^4)(\lambda_{\max}^2(\Sigma)\mathrm{tr}{(\Sigma)}^2+(\mu^T \mu)^4)\\
        =&
        o\Big(n^4{\big(\mathrm{tr}\Sigma^2\big)}^2\Big),
    \end{aligned}
    \end{equation*}
    where the last line follows by assumption~\eqref{chenC3} and~\eqref{myLoc}.
    Thus~\eqref{lemma2R1} holds.~\eqref{lemma2R2} and~\eqref{lemma2R3} follow by Lemma~\ref{smallLemma1} and Lemma~\ref{smallLemma2}.

\end{proof}

\begin{proof}[Proof of Theorem~\ref{shaziCLT}]
By a standard subsequence argument, we only need to prove 
    \begin{equation}\label{changchuan}
        \rho\Big(\mathcal{L}\Big(\frac{T_2(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big),N(0,1)\Big)\xrightarrow{a.s.} 0
    \end{equation}
     along a subsequence.
    But there is a subsequence $\{n(k)\}$ along which~\eqref{lemma2R1},~\eqref{lemma2R2} and~\eqref{lemma2R3} holds almost surely.
    By Proposition~\ref{CLTprop}, we have
    \begin{equation*}
        \mathcal{L}\Big(\frac{T_2(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}\Big|X_1,\ldots,X_n\Big)\xrightarrow{\mathcal{L}}N(0,1)
    \end{equation*}
    almost surely along $\{n(k)\}$, which means~\eqref{changchuan} holds along $\{n(k)\}$.

\end{proof}



\begin{proof}[Proof of Theorem~\ref{theoremPower}]
    By Theorem~\ref{theoremChen} and Lemma~\ref{ratioLemma}, we have
    \begin{equation}
        \frac{T_2(X_1,\ldots,X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\sum_{1\leq j< i\leq n}(X_i^T X_j)^2}}\xrightarrow{\mathcal{L}}N(0,1).
    \end{equation}
    By Corollary~\ref{corollaryQuan}, we have $\xi_{\alpha}^*\xrightarrow{P} \Phi(1-\alpha)$.
    Now the asymptotic power function can be derived by Slutsky's theorem
    \begin{equation}
        \begin{aligned}
            &\Pr\Big(\frac{T_2( X_1,\ldots, X_n)}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^* \Big)\\
            =&
            \Pr\Big(\frac{T_2( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}>\xi_{\alpha}^*-\frac{\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}} \Big)\\
            =&
            \Pr\Big(\frac{T_2( X_1,\ldots, X_n)-\frac{n(n-1)}{2}\mu^T\mu}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}-
            \frac{\sqrt{\sum_{1\leq j<i\leq n}{(X_i^T X_j)}^2}}{\sqrt{\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2}}\xi_{\alpha}^*>
            -\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}} \Big)\\
            =&
            \Pr\Big(N(0,1)-\Phi(1-\alpha)>-\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}}\Big)+o(1)\\
            =&
            \Phi(-\Phi(1-\alpha)+\frac{\sqrt{n(n-1)}\mu^T\mu}{\sqrt{2\mathrm{tr}\Sigma^2}})+o(1),
        \end{aligned}
    \end{equation}
    where the last two equality holds by Lemma~\ref{lemmaUniformSimple}.



\end{proof}

\begin{proof}[Proof of Theorem~\ref{farT}]
    \begin{equation*}
        \begin{aligned}
            \sum_{j<i} X_i^T X_j \epsilon_i\epsilon_j=&
            \sum_{j<i} Z_i^T \Gamma^T \Gamma Z_j \epsilon_i\epsilon_j\\
            &+
            \sum_{j<i} \mu^T \Gamma Z_i \epsilon_i\epsilon_j
            +\sum_{j<i} \mu^T \Gamma Z_j \epsilon_i\epsilon_j+
            \mu^T \mu \sum_{j<i} \epsilon_i\epsilon_j\\
            \overset{def}{=}&C_1+C_2+C_3+C_4.
        \end{aligned}
    \end{equation*}
Term $C_4$ plays a major role. Note that
    \begin{equation*}
        \begin{aligned}
            C_4=\frac{n}{2}\mu^T \mu\Big({\Big(\frac{1}{\sqrt{n}}\sum_{i=1}^n \epsilon_i\Big)}^2-1\Big).
        \end{aligned}
    \end{equation*}
By central limit theorem, we have
    \begin{equation*}
        \begin{aligned}
            \rho\Big(\mathcal{L}\Big(\frac{C_4}{\frac{n}{2}\mu^T \mu}\Big| X_1,\ldots,X_n\Big),\chi^2_1-1\Big)\xrightarrow{a.s.} 0.
        \end{aligned}
    \end{equation*}
Next we show that $C_1$, $C_2$ and $C_3$ are negligible under the assumptions of the theorem.
    By a standard subsequence argument and Slutsky's theorem, we can obtain
    \begin{equation}\label{rush2}
        \begin{aligned}
            \rho\Big(\mathcal{L}\Big(\frac{T(\epsilon_1 X_1,\ldots, \epsilon_i X_i,\ldots,\epsilon_n X_n)}{\frac{n}{2}\mu^T \mu}\Big| X_1,\ldots,X_n\Big),\chi^2_1-1\Big)\xrightarrow{P} 0
        \end{aligned}
    \end{equation}
    by showing that
    $$
    \mathrm{E}\Big({\Big(\frac{C_i}{\frac{n}{2}\mu^T\mu}\Big)}^2\Big|X_1,\ldots,X_n\Big)\xrightarrow{P} 0,\quad i=1,2,3.
    $$
    It in turn suffices to show

    \begin{equation}\label{Rush}
    \mathrm{E}{\Big(\frac{C_i}{\frac{n}{2}\mu^T\mu}\Big)}^2\to 0,
    \quad i=1,2,3.
    \end{equation}
    By direct calculation, we have
    $$\mathrm{E}(C_1^2)=\mathrm{E}\mathrm{E}(C_1^2|X_1,\ldots,X_n)=\sum_{j<i}\mathrm{E}{(Z_i^T \Gamma^T \Gamma Z_j)}^2=\frac{n(n-1)}{2}\mathrm{tr}\Sigma^2,$$
    and
    $$\mathrm{E}(C_2^2)=\mathrm{E}(C_3^2)=\frac{n(n-1)}{2}\mu^T \Sigma \mu\leq \frac{n(n-1)}{2}\sqrt{\mathrm{tr}\Sigma^2}\mu^T\mu.$$
    Thus~\eqref{Rush} follows by Assumption~\eqref{mu3}. Having~\eqref{rush2} holds, the theorem follows by Slutsky's theorem, Lemma~\ref{ratioLemma} and Assumption~\eqref{mu3}.
\end{proof}


\section*{References}

\bibliography{mybibfile}

\end{document}
